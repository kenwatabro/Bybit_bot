{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from 2024-02-13 00:00:00+09:19 to 2024-02-14 00:00:00+09:19\n",
      "Fetching data from 2024-02-14 00:00:00+09:19 to 2024-02-15 00:00:00+09:19\n",
      "Fetching data from 2024-02-15 00:00:00+09:19 to 2024-02-16 00:00:00+09:19\n",
      "Fetching data from 2024-02-16 00:00:00+09:19 to 2024-02-17 00:00:00+09:19\n",
      "Fetching data from 2024-02-17 00:00:00+09:19 to 2024-02-18 00:00:00+09:19\n",
      "Fetching data from 2024-02-18 00:00:00+09:19 to 2024-02-19 00:00:00+09:19\n",
      "Fetching data from 2024-02-19 00:00:00+09:19 to 2024-02-20 00:00:00+09:19\n",
      "Fetching data from 2024-02-20 00:00:00+09:19 to 2024-02-21 00:00:00+09:19\n",
      "Fetching data from 2024-02-21 00:00:00+09:19 to 2024-02-22 00:00:00+09:19\n",
      "Fetching data from 2024-02-22 00:00:00+09:19 to 2024-02-23 00:00:00+09:19\n",
      "Fetching data from 2024-02-23 00:00:00+09:19 to 2024-02-24 00:00:00+09:19\n",
      "Fetching data from 2024-02-24 00:00:00+09:19 to 2024-02-25 00:00:00+09:19\n",
      "Fetching data from 2024-02-25 00:00:00+09:19 to 2024-02-26 00:00:00+09:19\n",
      "Fetching data from 2024-02-26 00:00:00+09:19 to 2024-02-27 00:00:00+09:19\n",
      "Fetching data from 2024-02-27 00:00:00+09:19 to 2024-02-28 00:00:00+09:19\n",
      "Fetching data from 2024-02-28 00:00:00+09:19 to 2024-02-29 00:00:00+09:19\n",
      "Fetching data from 2024-02-29 00:00:00+09:19 to 2024-03-01 00:00:00+09:19\n",
      "Fetching data from 2024-03-01 00:00:00+09:19 to 2024-03-02 00:00:00+09:19\n",
      "Fetching data from 2024-03-02 00:00:00+09:19 to 2024-03-03 00:00:00+09:19\n",
      "Fetching data from 2024-03-03 00:00:00+09:19 to 2024-03-04 00:00:00+09:19\n",
      "Fetching data from 2024-03-04 00:00:00+09:19 to 2024-03-05 00:00:00+09:19\n",
      "Fetching data from 2024-03-05 00:00:00+09:19 to 2024-03-06 00:00:00+09:19\n",
      "Fetching data from 2024-03-06 00:00:00+09:19 to 2024-03-07 00:00:00+09:19\n",
      "Fetching data from 2024-03-07 00:00:00+09:19 to 2024-03-08 00:00:00+09:19\n",
      "Fetching data from 2024-03-08 00:00:00+09:19 to 2024-03-09 00:00:00+09:19\n",
      "Fetching data from 2024-03-09 00:00:00+09:19 to 2024-03-10 00:00:00+09:19\n",
      "Fetching data from 2024-03-10 00:00:00+09:19 to 2024-03-11 00:00:00+09:19\n",
      "Fetching data from 2024-03-11 00:00:00+09:19 to 2024-03-12 00:00:00+09:19\n",
      "Fetching data from 2024-03-12 00:00:00+09:19 to 2024-03-13 00:00:00+09:19\n",
      "Fetching data from 2024-03-13 00:00:00+09:19 to 2024-03-14 00:00:00+09:19\n",
      "Fetching data from 2024-03-14 00:00:00+09:19 to 2024-03-15 00:00:00+09:19\n",
      "Fetching data from 2024-03-15 00:00:00+09:19 to 2024-03-16 00:00:00+09:19\n",
      "Fetching data from 2024-03-16 00:00:00+09:19 to 2024-03-17 00:00:00+09:19\n",
      "Fetching data from 2024-03-17 00:00:00+09:19 to 2024-03-18 00:00:00+09:19\n",
      "Fetching data from 2024-03-18 00:00:00+09:19 to 2024-03-19 00:00:00+09:19\n",
      "Fetching data from 2024-03-19 00:00:00+09:19 to 2024-03-20 00:00:00+09:19\n",
      "Fetching data from 2024-03-20 00:00:00+09:19 to 2024-03-21 00:00:00+09:19\n",
      "Fetching data from 2024-03-21 00:00:00+09:19 to 2024-03-22 00:00:00+09:19\n",
      "Fetching data from 2024-03-22 00:00:00+09:19 to 2024-03-23 00:00:00+09:19\n",
      "Fetching data from 2024-03-23 00:00:00+09:19 to 2024-03-24 00:00:00+09:19\n",
      "Fetching data from 2024-03-24 00:00:00+09:19 to 2024-03-25 00:00:00+09:19\n",
      "Fetching data from 2024-03-25 00:00:00+09:19 to 2024-03-26 00:00:00+09:19\n",
      "Fetching data from 2024-03-26 00:00:00+09:19 to 2024-03-27 00:00:00+09:19\n",
      "Fetching data from 2024-03-27 00:00:00+09:19 to 2024-03-28 00:00:00+09:19\n",
      "Fetching data from 2024-03-28 00:00:00+09:19 to 2024-03-29 00:00:00+09:19\n",
      "Fetching data from 2024-03-29 00:00:00+09:19 to 2024-03-30 00:00:00+09:19\n",
      "Fetching data from 2024-03-30 00:00:00+09:19 to 2024-03-31 00:00:00+09:19\n",
      "Fetching data from 2024-03-31 00:00:00+09:19 to 2024-04-01 00:00:00+09:19\n",
      "Fetching data from 2024-04-01 00:00:00+09:19 to 2024-04-02 00:00:00+09:19\n",
      "Fetching data from 2024-04-02 00:00:00+09:19 to 2024-04-03 00:00:00+09:19\n",
      "Fetching data from 2024-04-03 00:00:00+09:19 to 2024-04-04 00:00:00+09:19\n",
      "Fetching data from 2024-04-04 00:00:00+09:19 to 2024-04-05 00:00:00+09:19\n",
      "Fetching data from 2024-04-05 00:00:00+09:19 to 2024-04-06 00:00:00+09:19\n",
      "Fetching data from 2024-04-06 00:00:00+09:19 to 2024-04-07 00:00:00+09:19\n",
      "Fetching data from 2024-04-07 00:00:00+09:19 to 2024-04-08 00:00:00+09:19\n",
      "Fetching data from 2024-04-08 00:00:00+09:19 to 2024-04-09 00:00:00+09:19\n",
      "Fetching data from 2024-04-09 00:00:00+09:19 to 2024-04-10 00:00:00+09:19\n",
      "Fetching data from 2024-04-10 00:00:00+09:19 to 2024-04-11 00:00:00+09:19\n",
      "Fetching data from 2024-04-11 00:00:00+09:19 to 2024-04-12 00:00:00+09:19\n",
      "Fetching data from 2024-04-12 00:00:00+09:19 to 2024-04-13 00:00:00+09:19\n",
      "Fetching data from 2024-04-13 00:00:00+09:19 to 2024-04-14 00:00:00+09:19\n",
      "Fetching data from 2024-04-14 00:00:00+09:19 to 2024-04-15 00:00:00+09:19\n",
      "Fetching data from 2024-04-15 00:00:00+09:19 to 2024-04-16 00:00:00+09:19\n",
      "Fetching data from 2024-04-16 00:00:00+09:19 to 2024-04-17 00:00:00+09:19\n",
      "Fetching data from 2024-04-17 00:00:00+09:19 to 2024-04-18 00:00:00+09:19\n",
      "Fetching data from 2024-04-18 00:00:00+09:19 to 2024-04-19 00:00:00+09:19\n",
      "Fetching data from 2024-04-19 00:00:00+09:19 to 2024-04-20 00:00:00+09:19\n",
      "Fetching data from 2024-04-20 00:00:00+09:19 to 2024-04-21 00:00:00+09:19\n",
      "Fetching data from 2024-04-21 00:00:00+09:19 to 2024-04-22 00:00:00+09:19\n",
      "Fetching data from 2024-04-22 00:00:00+09:19 to 2024-04-23 00:00:00+09:19\n",
      "Fetching data from 2024-04-23 00:00:00+09:19 to 2024-04-24 00:00:00+09:19\n",
      "Fetching data from 2024-04-24 00:00:00+09:19 to 2024-04-25 00:00:00+09:19\n",
      "Fetching data from 2024-04-25 00:00:00+09:19 to 2024-04-26 00:00:00+09:19\n",
      "Fetching data from 2024-04-26 00:00:00+09:19 to 2024-04-27 00:00:00+09:19\n",
      "Fetching data from 2024-04-27 00:00:00+09:19 to 2024-04-28 00:00:00+09:19\n",
      "Fetching data from 2024-04-28 00:00:00+09:19 to 2024-04-29 00:00:00+09:19\n",
      "Fetching data from 2024-04-29 00:00:00+09:19 to 2024-04-30 00:00:00+09:19\n",
      "Fetching data from 2024-04-30 00:00:00+09:19 to 2024-05-01 00:00:00+09:19\n",
      "Fetching data from 2024-05-01 00:00:00+09:19 to 2024-05-02 00:00:00+09:19\n",
      "Fetching data from 2024-05-02 00:00:00+09:19 to 2024-05-03 00:00:00+09:19\n",
      "Fetching data from 2024-05-03 00:00:00+09:19 to 2024-05-04 00:00:00+09:19\n",
      "Fetching data from 2024-05-04 00:00:00+09:19 to 2024-05-05 00:00:00+09:19\n",
      "Fetching data from 2024-05-05 00:00:00+09:19 to 2024-05-06 00:00:00+09:19\n",
      "Fetching data from 2024-05-06 00:00:00+09:19 to 2024-05-07 00:00:00+09:19\n",
      "Fetching data from 2024-05-07 00:00:00+09:19 to 2024-05-08 00:00:00+09:19\n",
      "Fetching data from 2024-05-08 00:00:00+09:19 to 2024-05-09 00:00:00+09:19\n",
      "Fetching data from 2024-05-09 00:00:00+09:19 to 2024-05-10 00:00:00+09:19\n",
      "Fetching data from 2024-05-10 00:00:00+09:19 to 2024-05-11 00:00:00+09:19\n",
      "Fetching data from 2024-05-11 00:00:00+09:19 to 2024-05-12 00:00:00+09:19\n",
      "Fetching data from 2024-05-12 00:00:00+09:19 to 2024-05-13 00:00:00+09:19\n",
      "Fetching data from 2024-05-13 00:00:00+09:19 to 2024-05-14 00:00:00+09:19\n",
      "Fetching data from 2024-05-14 00:00:00+09:19 to 2024-05-15 00:00:00+09:19\n",
      "Fetching data from 2024-05-15 00:00:00+09:19 to 2024-05-16 00:00:00+09:19\n",
      "Fetching data from 2024-05-16 00:00:00+09:19 to 2024-05-17 00:00:00+09:19\n",
      "Fetching data from 2024-05-17 00:00:00+09:19 to 2024-05-18 00:00:00+09:19\n",
      "Fetching data from 2024-05-18 00:00:00+09:19 to 2024-05-19 00:00:00+09:19\n",
      "Fetching data from 2024-05-19 00:00:00+09:19 to 2024-05-20 00:00:00+09:19\n",
      "Fetching data from 2024-05-20 00:00:00+09:19 to 2024-05-21 00:00:00+09:19\n",
      "Fetching data from 2024-05-21 00:00:00+09:19 to 2024-05-22 00:00:00+09:19\n",
      "Fetching data from 2024-05-22 00:00:00+09:19 to 2024-05-23 00:00:00+09:19\n",
      "Fetching data from 2024-05-23 00:00:00+09:19 to 2024-05-24 00:00:00+09:19\n",
      "Fetching data from 2024-05-24 00:00:00+09:19 to 2024-05-25 00:00:00+09:19\n",
      "Fetching data from 2024-05-25 00:00:00+09:19 to 2024-05-26 00:00:00+09:19\n",
      "Fetching data from 2024-05-26 00:00:00+09:19 to 2024-05-27 00:00:00+09:19\n",
      "Fetching data from 2024-05-27 00:00:00+09:19 to 2024-05-28 00:00:00+09:19\n",
      "Fetching data from 2024-05-28 00:00:00+09:19 to 2024-05-29 00:00:00+09:19\n",
      "Fetching data from 2024-05-29 00:00:00+09:19 to 2024-05-30 00:00:00+09:19\n",
      "Fetching data from 2024-05-30 00:00:00+09:19 to 2024-05-31 00:00:00+09:19\n",
      "Fetching data from 2024-05-31 00:00:00+09:19 to 2024-06-01 00:00:00+09:19\n",
      "Fetching data from 2024-06-01 00:00:00+09:19 to 2024-06-02 00:00:00+09:19\n",
      "Fetching data from 2024-06-02 00:00:00+09:19 to 2024-06-03 00:00:00+09:19\n",
      "Fetching data from 2024-06-03 00:00:00+09:19 to 2024-06-04 00:00:00+09:19\n",
      "Fetching data from 2024-06-04 00:00:00+09:19 to 2024-06-05 00:00:00+09:19\n",
      "Fetching data from 2024-06-05 00:00:00+09:19 to 2024-06-06 00:00:00+09:19\n",
      "Fetching data from 2024-06-06 00:00:00+09:19 to 2024-06-07 00:00:00+09:19\n",
      "Fetching data from 2024-06-07 00:00:00+09:19 to 2024-06-08 00:00:00+09:19\n",
      "Fetching data from 2024-06-08 00:00:00+09:19 to 2024-06-09 00:00:00+09:19\n",
      "Fetching data from 2024-06-09 00:00:00+09:19 to 2024-06-10 00:00:00+09:19\n",
      "Fetching data from 2024-06-10 00:00:00+09:19 to 2024-06-11 00:00:00+09:19\n",
      "Fetching data from 2024-06-11 00:00:00+09:19 to 2024-06-12 00:00:00+09:19\n",
      "Fetching data from 2024-06-12 00:00:00+09:19 to 2024-06-13 00:00:00+09:19\n",
      "Fetching data from 2024-06-13 00:00:00+09:19 to 2024-06-14 00:00:00+09:19\n",
      "Fetching data from 2024-06-14 00:00:00+09:19 to 2024-06-15 00:00:00+09:19\n",
      "Fetching data from 2024-06-15 00:00:00+09:19 to 2024-06-16 00:00:00+09:19\n",
      "Fetching data from 2024-06-16 00:00:00+09:19 to 2024-06-17 00:00:00+09:19\n",
      "Fetching data from 2024-06-17 00:00:00+09:19 to 2024-06-18 00:00:00+09:19\n",
      "Fetching data from 2024-06-18 00:00:00+09:19 to 2024-06-19 00:00:00+09:19\n",
      "Fetching data from 2024-06-19 00:00:00+09:19 to 2024-06-20 00:00:00+09:19\n",
      "Fetching data from 2024-06-20 00:00:00+09:19 to 2024-06-21 00:00:00+09:19\n",
      "Fetching data from 2024-06-21 00:00:00+09:19 to 2024-06-22 00:00:00+09:19\n",
      "Fetching data from 2024-06-22 00:00:00+09:19 to 2024-06-23 00:00:00+09:19\n",
      "Fetching data from 2024-06-23 00:00:00+09:19 to 2024-06-24 00:00:00+09:19\n",
      "Fetching data from 2024-06-24 00:00:00+09:19 to 2024-06-25 00:00:00+09:19\n",
      "Fetching data from 2024-06-25 00:00:00+09:19 to 2024-06-26 00:00:00+09:19\n",
      "Fetching data from 2024-06-26 00:00:00+09:19 to 2024-06-27 00:00:00+09:19\n",
      "Fetching data from 2024-06-27 00:00:00+09:19 to 2024-06-28 00:00:00+09:19\n",
      "Fetching data from 2024-06-28 00:00:00+09:19 to 2024-06-29 00:00:00+09:19\n",
      "Fetching data from 2024-06-29 00:00:00+09:19 to 2024-06-30 00:00:00+09:19\n",
      "Fetching data from 2024-06-30 00:00:00+09:19 to 2024-07-01 00:00:00+09:19\n",
      "Fetching data from 2024-07-01 00:00:00+09:19 to 2024-07-02 00:00:00+09:19\n",
      "Fetching data from 2024-07-02 00:00:00+09:19 to 2024-07-03 00:00:00+09:19\n",
      "Fetching data from 2024-07-03 00:00:00+09:19 to 2024-07-04 00:00:00+09:19\n",
      "Fetching data from 2024-07-04 00:00:00+09:19 to 2024-07-05 00:00:00+09:19\n",
      "Fetching data from 2024-07-05 00:00:00+09:19 to 2024-07-06 00:00:00+09:19\n",
      "Fetching data from 2024-07-06 00:00:00+09:19 to 2024-07-07 00:00:00+09:19\n",
      "Fetching data from 2024-07-07 00:00:00+09:19 to 2024-07-08 00:00:00+09:19\n",
      "Fetching data from 2024-07-08 00:00:00+09:19 to 2024-07-09 00:00:00+09:19\n",
      "Fetching data from 2024-07-09 00:00:00+09:19 to 2024-07-10 00:00:00+09:19\n",
      "Fetching data from 2024-07-10 00:00:00+09:19 to 2024-07-11 00:00:00+09:19\n",
      "Fetching data from 2024-07-11 00:00:00+09:19 to 2024-07-12 00:00:00+09:19\n",
      "Fetching data from 2024-07-12 00:00:00+09:19 to 2024-07-13 00:00:00+09:19\n",
      "Fetching data from 2024-07-13 00:00:00+09:19 to 2024-07-14 00:00:00+09:19\n",
      "Fetching data from 2024-07-14 00:00:00+09:19 to 2024-07-15 00:00:00+09:19\n",
      "Fetching data from 2024-07-15 00:00:00+09:19 to 2024-07-16 00:00:00+09:19\n",
      "Fetching data from 2024-07-16 00:00:00+09:19 to 2024-07-17 00:00:00+09:19\n",
      "Fetching data from 2024-07-17 00:00:00+09:19 to 2024-07-18 00:00:00+09:19\n",
      "Fetching data from 2024-07-18 00:00:00+09:19 to 2024-07-19 00:00:00+09:19\n",
      "Fetching data from 2024-07-19 00:00:00+09:19 to 2024-07-20 00:00:00+09:19\n",
      "Fetching data from 2024-07-20 00:00:00+09:19 to 2024-07-21 00:00:00+09:19\n",
      "Fetching data from 2024-07-21 00:00:00+09:19 to 2024-07-22 00:00:00+09:19\n",
      "Fetching data from 2024-07-22 00:00:00+09:19 to 2024-07-23 00:00:00+09:19\n",
      "Fetching data from 2024-07-23 00:00:00+09:19 to 2024-07-24 00:00:00+09:19\n",
      "Fetching data from 2024-07-24 00:00:00+09:19 to 2024-07-25 00:00:00+09:19\n",
      "Fetching data from 2024-07-25 00:00:00+09:19 to 2024-07-26 00:00:00+09:19\n",
      "Fetching data from 2024-07-26 00:00:00+09:19 to 2024-07-27 00:00:00+09:19\n",
      "Fetching data from 2024-07-27 00:00:00+09:19 to 2024-07-28 00:00:00+09:19\n",
      "Fetching data from 2024-07-28 00:00:00+09:19 to 2024-07-29 00:00:00+09:19\n",
      "Fetching data from 2024-07-29 00:00:00+09:19 to 2024-07-30 00:00:00+09:19\n",
      "Fetching data from 2024-07-30 00:00:00+09:19 to 2024-07-31 00:00:00+09:19\n",
      "Fetching data from 2024-07-31 00:00:00+09:19 to 2024-08-01 00:00:00+09:19\n",
      "Fetching data from 2024-08-01 00:00:00+09:19 to 2024-08-02 00:00:00+09:19\n",
      "Fetching data from 2024-08-02 00:00:00+09:19 to 2024-08-03 00:00:00+09:19\n",
      "Fetching data from 2024-08-03 00:00:00+09:19 to 2024-08-04 00:00:00+09:19\n",
      "Fetching data from 2024-08-04 00:00:00+09:19 to 2024-08-05 00:00:00+09:19\n",
      "Fetching data from 2024-08-05 00:00:00+09:19 to 2024-08-06 00:00:00+09:19\n",
      "Fetching data from 2024-08-06 00:00:00+09:19 to 2024-08-07 00:00:00+09:19\n",
      "Fetching data from 2024-08-07 00:00:00+09:19 to 2024-08-08 00:00:00+09:19\n",
      "Fetching data from 2024-08-08 00:00:00+09:19 to 2024-08-09 00:00:00+09:19\n",
      "Fetching data from 2024-08-09 00:00:00+09:19 to 2024-08-10 00:00:00+09:19\n",
      "Fetching data from 2024-08-10 00:00:00+09:19 to 2024-08-11 00:00:00+09:19\n",
      "Total rows: 51840\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pytz\n",
    "\n",
    "BACKTEST_DATA_FOLDER = \"backtest_data\"\n",
    "\n",
    "\n",
    "def get_binance_klines(symbol, interval, start_time, end_time):\n",
    "    url = \"https://api.binance.com/api/v3/klines\"\n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"interval\": interval,\n",
    "        \"startTime\": int(start_time.timestamp() * 1000),\n",
    "        \"endTime\": int(end_time.timestamp() * 1000),\n",
    "        \"limit\": 1000,\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\n",
    "            \"timestamp\",\n",
    "            \"open\",\n",
    "            \"high\",\n",
    "            \"low\",\n",
    "            \"close\",\n",
    "            \"volume\",\n",
    "            \"close_time\",\n",
    "            \"quote_asset_volume\",\n",
    "            \"number_of_trades\",\n",
    "            \"taker_buy_base_asset_volume\",\n",
    "            \"taker_buy_quote_asset_volume\",\n",
    "            \"ignore\",\n",
    "        ],\n",
    "    )\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\", utc=True)\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].dt.tz_convert(\"Asia/Tokyo\")  # UTCからJSTに変換\n",
    "    return df[[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "\n",
    "\n",
    "def get_historical_data(symbol, interval, start_date, end_date):\n",
    "    all_data = []\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        next_date = min(current_date + timedelta(days=1), end_date)\n",
    "        print(f\"Fetching data from {current_date} to {next_date}\")\n",
    "        df = get_binance_klines(symbol, interval, current_date, next_date)\n",
    "        if df.empty:\n",
    "            print(f\"No data found for {symbol} in the specified time range.\")\n",
    "            exit()\n",
    "        all_data.append(df)\n",
    "        current_date = next_date\n",
    "        time.sleep(1)  # APIレート制限を考慮\n",
    "    return pd.concat(all_data)\n",
    "\n",
    "\n",
    "# パラメータ設定\n",
    "symbol = \"SOLUSDT\"\n",
    "interval = \"5m\"\n",
    "end_date = datetime(\n",
    "    2024, 8, 11, tzinfo=pytz.timezone(\"Asia/Tokyo\")\n",
    ")  # 固定の終了日（JST）\n",
    "start_date = end_date - timedelta(days=180)\n",
    "\n",
    "# データ取得\n",
    "historical_data = get_historical_data(symbol, interval, start_date, end_date)\n",
    "\n",
    "# CSVに保存\n",
    "historical_data.to_csv(\n",
    "    f\"{BACKTEST_DATA_FOLDER}/{symbol}_{interval}_{start_date.date()}_{end_date.date()}_JST.csv\", index=False\n",
    ")\n",
    "\n",
    "print(f\"Total rows: {len(historical_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = pd.read_csv(\"ETHUSDT_5m_2024-02-13_2024-08-11_JST.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  timestamp          open          high  \\\n",
      "count                                 51840         51840         51840   \n",
      "unique                                  NaN          9295          9264   \n",
      "top                                     NaN  145.00000000  145.00000000   \n",
      "freq                                    NaN            32            37   \n",
      "mean    2024-05-12 23:42:29.999999744+09:00           NaN           NaN   \n",
      "min               2024-02-12 23:45:00+09:00           NaN           NaN   \n",
      "25%               2024-03-28 23:43:45+09:00           NaN           NaN   \n",
      "50%               2024-05-12 23:42:30+09:00           NaN           NaN   \n",
      "75%               2024-06-26 23:41:15+09:00           NaN           NaN   \n",
      "max               2024-08-10 23:40:00+09:00           NaN           NaN   \n",
      "\n",
      "                 low         close          volume  \n",
      "count          51840         51840           51840  \n",
      "unique          9259          9322           51725  \n",
      "top     146.00000000  145.00000000  10645.68400000  \n",
      "freq              45            30               2  \n",
      "mean             NaN           NaN             NaN  \n",
      "min              NaN           NaN             NaN  \n",
      "25%              NaN           NaN             NaN  \n",
      "50%              NaN           NaN             NaN  \n",
      "75%              NaN           NaN             NaN  \n",
      "max              NaN           NaN             NaN  \n",
      "\n",
      "First 10 trades:\n",
      "[np.float64(0.017722640673460348), np.float64(-0.0015040254799610873), np.float64(0.013197619252997384), np.float64(0.0157679240351946), np.float64(0.015124712542013035), np.float64(0.02374977594551), np.float64(0.009431395665185511), np.float64(0.01770564367392108), np.float64(0.015902196893096698), np.float64(0.02205466540999061)]\n",
      "\n",
      "Last 10 trades:\n",
      "[np.float64(0.02901561348254381), np.float64(0.03034584847261466), np.float64(0.03529327713081373), np.float64(0.06195495563812836), np.float64(-0.0034768866060845335), np.float64(0.09959023694993757), np.float64(0.07787304334146154), np.float64(0.021251241310824093), np.float64(0.06902109172447617), np.float64(0.02134204960584511)]\n",
      "\n",
      "Backtesting Results:\n",
      "Total Return: 620.24%\n",
      "Average Trade Return: 2.19%\n",
      "Win Rate: 97.53%\n",
      "Sharpe Ratio: 19.25\n",
      "Number of Trades: 283\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.utils.indicators import calculate_rsi, calculate_bollinger_bands\n",
    "\n",
    "\n",
    "def calculate_performance(trades):\n",
    "    if not trades:\n",
    "        return 0, 0, 0, 0\n",
    "    total_return = sum(trades)\n",
    "    avg_return = total_return / len(trades)\n",
    "    win_rate = sum(1 for t in trades if t > 0) / len(trades)\n",
    "    returns = np.array(trades)\n",
    "    sharpe_ratio = (returns.mean() / returns.std()) * np.sqrt(252)  # Annualized\n",
    "    return total_return, avg_return, win_rate, sharpe_ratio\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    # Reset index to ensure it's unique\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Convert relevant columns to numeric types\n",
    "    numeric_columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "    for col in numeric_columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Remove existing RSI column if it exists\n",
    "    if \"rsi\" in df.columns:\n",
    "        df = df.drop(\"rsi\", axis=1)\n",
    "\n",
    "    # Drop any rows with NaN values after conversion\n",
    "    df = df.dropna(subset=numeric_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def backtest_rsi_bollinger_strategy(\n",
    "    df, rsi_period=14, bb_period=20, bb_std=2, rsi_oversold=35, rsi_overbought=70\n",
    "):\n",
    "    # Prepare the data\n",
    "    df = prepare_data(df)\n",
    "\n",
    "    # Calculate indicators\n",
    "    df[\"rsi\"] = calculate_rsi(df[\"close\"].values, period=rsi_period)\n",
    "\n",
    "    # Calculate Bollinger Bands\n",
    "    bb_upper, bb_middle, bb_lower = calculate_bollinger_bands(\n",
    "        df[\"close\"].values, period=bb_period, num_std_dev=bb_std\n",
    "    )\n",
    "\n",
    "    # Add Bollinger Bands to the DataFrame, aligning with the index\n",
    "    df[\"bb_upper\"] = pd.Series(bb_upper, index=df.index[: len(bb_upper)])\n",
    "    df[\"bb_middle\"] = pd.Series(bb_middle, index=df.index[: len(bb_middle)])\n",
    "    df[\"bb_lower\"] = pd.Series(bb_lower, index=df.index[: len(bb_lower)])\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Initialize variables\n",
    "    position = None\n",
    "    entry_price = 0\n",
    "    trades = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if i < max(rsi_period, bb_period):\n",
    "            continue  # Skip until we have enough data for indicators\n",
    "\n",
    "        row = df.iloc[i]\n",
    "        if position is None:\n",
    "            if row[\"rsi\"] < rsi_oversold and row[\"close\"] < row[\"bb_lower\"]:\n",
    "                position = \"long\"\n",
    "                entry_price = row[\"close\"]\n",
    "        elif position == \"long\":\n",
    "            if row[\"rsi\"] > rsi_overbought or row[\"close\"] > row[\"bb_upper\"]:\n",
    "                profit = (row[\"close\"] - entry_price) / entry_price\n",
    "                trades.append(profit)\n",
    "                position = None\n",
    "\n",
    "    return trades\n",
    "\n",
    "\n",
    "# Use the function\n",
    "trades = backtest_rsi_bollinger_strategy(historical_data)\n",
    "\n",
    "# Calculate performance metrics\n",
    "total_return, avg_return, win_rate, sharpe_ratio = calculate_performance(trades)\n",
    "\n",
    "print(historical_data.describe())\n",
    "\n",
    "# トレード詳細の表示\n",
    "print(\"\\nFirst 10 trades:\")\n",
    "print(trades[:10])\n",
    "print(\"\\nLast 10 trades:\")\n",
    "print(trades[-10:])\n",
    "\n",
    "print(f\"\\nBacktesting Results:\")\n",
    "print(f\"Total Return: {total_return:.2%}\")\n",
    "print(f\"Average Trade Return: {avg_return:.2%}\")\n",
    "print(f\"Win Rate: {win_rate:.2%}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "print(f\"Number of Trades: {len(trades)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 51840 entries, 0 to 287\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype                     \n",
      "---  ------     --------------  -----                     \n",
      " 0   timestamp  51840 non-null  datetime64[ns, Asia/Tokyo]\n",
      " 1   open       51840 non-null  object                    \n",
      " 2   high       51840 non-null  object                    \n",
      " 3   low        51840 non-null  object                    \n",
      " 4   close      51840 non-null  float64                   \n",
      " 5   volume     51840 non-null  object                    \n",
      " 6   rsi        51840 non-null  float64                   \n",
      "dtypes: datetime64[ns, Asia/Tokyo](1), float64(2), object(4)\n",
      "memory usage: 3.2+ MB\n",
      "None\n",
      "                  timestamp            open            high             low  \\\n",
      "0 2024-02-12 23:45:00+09:00  48556.30000000  48719.17000000  48455.51000000   \n",
      "1 2024-02-12 23:50:00+09:00  48606.20000000  48663.20000000  48520.00000000   \n",
      "2 2024-02-12 23:55:00+09:00  48623.66000000  48798.00000000  48595.00000000   \n",
      "3 2024-02-13 00:00:00+09:00  48797.99000000  48867.00000000  48669.39000000   \n",
      "4 2024-02-13 00:05:00+09:00  48866.80000000  48980.00000000  48738.85000000   \n",
      "\n",
      "      close         volume        rsi  \n",
      "0  48606.20   716.71369000  78.303395  \n",
      "1  48623.66   456.61425000  78.303395  \n",
      "2  48798.00   711.98091000  78.303395  \n",
      "3  48866.80   709.38659000  78.303395  \n",
      "4  48972.00  1127.47518000  78.303395  \n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(historical_data.info())\n",
    "print(historical_data.head())\n",
    "print(historical_data.index.is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bybit_tradingbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
